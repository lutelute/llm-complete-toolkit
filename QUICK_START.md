# ðŸš€ LLM Complete Toolkit ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰

## ðŸ“‹ æ¦‚è¦
ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€LLM Complete Toolkitã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ã‚’5ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã§èª¬æ˜Žã—ã¾ã™ã€‚

## ðŸŽ¯ å‰ææ¡ä»¶
- Python 3.8ä»¥ä¸Š
- 8GBä»¥ä¸Šã®RAMï¼ˆæŽ¨å¥¨ï¼š16GBä»¥ä¸Šï¼‰
- GPUï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€CUDAå¯¾å¿œã¾ãŸã¯MPSå¯¾å¿œï¼‰

## ðŸ”§ ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆæŽ¨å¥¨ï¼‰
```bash
# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/lutelute/llm-complete-toolkit.git
cd llm-complete-toolkit

# è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Ÿè¡Œ
./install.sh  # Linux/macOS
# ã¾ãŸã¯
install.bat   # Windows
```

### æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
# ä»®æƒ³ç’°å¢ƒã®ä½œæˆ
python -m venv venv
source venv/bin/activate  # Linux/macOS
# venv\Scripts\activate   # Windows

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt
python setup.py
```

## ðŸ“¥ ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

```bash
# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’ç¢ºèª
python scripts/download_models.py --list-popular

# è»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›žæŽ¨å¥¨ï¼‰
python scripts/download_models.py --model microsoft/DialoGPT-small

# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
python scripts/download_models.py --model gpt2 --model microsoft/DialoGPT-medium
```

## ðŸ“„ ã‚¹ãƒ†ãƒƒãƒ—3: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™

```bash
# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
python main.py create-samples

# ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
ls data/
# sample_document.md  train.jsonl  eval.jsonl
```

## ðŸ“Š ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†

```bash
# PDFã‚„Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
python main.py extract data/ outputs/ --format jsonl --instruction-format

# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
ls outputs/
# training_data.jsonl  instruction_data.jsonl
```

## ðŸ¤– ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

### A. LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
```bash
# LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
python main.py train-lora --train-data data/train.jsonl --eval-data data/eval.jsonl

# é«˜é€ŸåŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
python main.py train-lora --train-data data/train.jsonl --batch-size 2 --epochs 1
```

### B. QLoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡ï¼‰
```bash
# QLoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
python main.py train-qlora --train-data data/train.jsonl --batch-size 1

# å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
python main.py train-qlora --train-data data/train.jsonl --model-name microsoft/DialoGPT-large
```

### C. å¼·åŒ–å­¦ç¿’
```bash
# PPOå¼·åŒ–å­¦ç¿’
python main.py train-rl --algorithm ppo --episodes 500

# DQNå¼·åŒ–å­¦ç¿’
python main.py train-rl --algorithm dqn --episodes 500
```

## ðŸ“ˆ ã‚¹ãƒ†ãƒƒãƒ—6: çµæžœã®ç¢ºèª

### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµæžœã®ç¢ºèª
```bash
# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
ls outputs/
ls models/fine_tuned_models/
ls models/trained_models/
```

### TensorBoardã§ã®å¯è¦–åŒ–
```bash
# TensorBoardã®èµ·å‹•
tensorboard --logdir=outputs/logs/tensorboard

# ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:6006 ã‚’é–‹ã
```

## ðŸŽ® å¿œç”¨ä¾‹

### 1. ã‚«ã‚¹ã‚¿ãƒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‡¦ç†
```bash
# ç‹¬è‡ªã®PDF/Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
mkdir my_documents
# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’my_documentsã«é…ç½®
python main.py extract my_documents/ my_outputs/ --chunk-size 1024
```

### 2. è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
```bash
# ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
python main.py train-lora --train-data data/train.jsonl --model-name gpt2
python main.py train-lora --train-data data/train.jsonl --model-name microsoft/DialoGPT-medium
```

### 3. ãƒãƒƒãƒå‡¦ç†ã§ã®è‡ªå‹•åŒ–
```bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat > batch_training.sh << 'EOF'
#!/bin/bash
python main.py extract data/ outputs/
python main.py train-lora --train-data outputs/instruction_data.jsonl --epochs 3
python main.py train-qlora --train-data outputs/instruction_data.jsonl --epochs 2
EOF

chmod +x batch_training.sh
./batch_training.sh
```

## ðŸ“ è¨­å®šã®ã‚«ã‚¹ã‚¿ãƒžã‚¤ã‚º

### config.yamlã®ç·¨é›†
```bash
# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†
nano configs/config.yaml

# ä¸»è¦ãªè¨­å®šé …ç›®ï¼š
# - device: "cuda" / "cpu" / "mps"
# - batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
# - learning_rate: å­¦ç¿’çŽ‡
# - epochs: ã‚¨ãƒãƒƒã‚¯æ•°
```

### ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
```bash
# GPUãƒ¡ãƒ¢ãƒªåˆ¶é™
export CUDA_VISIBLE_DEVICES=0

# HuggingFaceã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
export HF_HOME=/path/to/cache

# Weights & Biasesè¨­å®š
export WANDB_PROJECT=my-llm-project
```

## ðŸ” æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ç¢ºèª**
   - `README.md`: å…¨ä½“çš„ãªæ¦‚è¦
   - `TECHNICAL_SPECIFICATIONS.md`: æŠ€è¡“ä»•æ§˜
   - `USAGE_EXAMPLES.md`: ä½¿ç”¨ä¾‹

2. **ã‚«ã‚¹ã‚¿ãƒžã‚¤ã‚ºã®å­¦ç¿’**
   - `configs/config.yaml`: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
   - `models/*/README.md`: ãƒ¢ãƒ‡ãƒ«åˆ¥ã®è©³ç´°

3. **é«˜åº¦ãªæ©Ÿèƒ½ã®æ´»ç”¨**
   - ã‚«ã‚¹ã‚¿ãƒ ç’°å¢ƒã®ä½œæˆ
   - ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
   - è¤‡æ•°GPUç’°å¢ƒã§ã®å­¦ç¿’

## â“ ã‚ˆãã‚ã‚‹è³ªå•

### Q1: ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™
```bash
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
python main.py train-lora --train-data data/train.jsonl --batch-size 1

# QLoRAã‚’ä½¿ç”¨ã™ã‚‹
python main.py train-qlora --train-data data/train.jsonl
```

### Q2: GPUãŒèªè­˜ã•ã‚Œã¾ã›ã‚“
```bash
# PyTorchã®GPUã‚µãƒãƒ¼ãƒˆã‚’ç¢ºèª
python -c "import torch; print(torch.cuda.is_available())"

# å¿…è¦ã«å¿œã˜ã¦CUDAå¯¾å¿œPyTorchã‚’å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

### Q3: å­¦ç¿’ãŒé…ã„ã§ã™
```bash
# Flash Attentionã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install flash-attn --no-build-isolation

# æ··åˆç²¾åº¦å­¦ç¿’ã‚’æœ‰åŠ¹åŒ–
python main.py train-lora --train-data data/train.jsonl --fp16
```

## ðŸš¨ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š

1. **Pythonç’°å¢ƒ**: `python --version` (3.8ä»¥ä¸Š)
2. **ä»®æƒ³ç’°å¢ƒ**: `which python` (venvå†…ã‹ç¢ºèª)
3. **GPUçŠ¶æ…‹**: `nvidia-smi` (CUDAç’°å¢ƒã®å ´åˆ)
4. **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: `python -c "import psutil; print(psutil.virtual_memory())"`

è©³ç´°ãªãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¯ `README.md` ã®ã€Œãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚