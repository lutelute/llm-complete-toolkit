# ğŸš€ LLM Complete Toolkit

PDFã‚„Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã¨ã€å¼·åŒ–å­¦ç¿’ãƒ»è»¢ç§»å­¦ç¿’ã‚’ç”¨ã„ãŸLLMãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®çµ±åˆãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆ

## âœ¨ ä¸»ãªç‰¹å¾´

### ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†
- **PDFè§£æ**: PDFplumber + PyPDF2ã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
- **Markdownè§£æ**: ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼å¯¾å¿œã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰²
- **LM Studioå¤‰æ›**: JSONLå½¢å¼ã§ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
- **è‡ªå‹•ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²**: æœ€é©ãªã‚µã‚¤ã‚ºã§ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²

### ğŸ§  è»¢ç§»å­¦ç¿’
- **LoRA**: åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- **QLoRA**: é‡å­åŒ–ã«ã‚ˆã‚‹è¶…ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
- **è‡ªå‹•ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™**: ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³å½¢å¼å¯¾å¿œ
- **HuggingFaceçµ±åˆ**: æœ€æ–°ã®Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªå¯¾å¿œ

### ğŸ¯ å¼·åŒ–å­¦ç¿’
- **PPO**: å®‰å®šã—ãŸãƒãƒªã‚·ãƒ¼å‹¾é…æ‰‹æ³•
- **DQN**: ä¾¡å€¤ãƒ™ãƒ¼ã‚¹å­¦ç¿’ï¼ˆDueling DQNå¯¾å¿œï¼‰
- **ã‚«ã‚¹ã‚¿ãƒ ç’°å¢ƒ**: ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ç’°å¢ƒã®ã‚µãƒ³ãƒ—ãƒ«å®Ÿè£…
- **TensorBoardçµ±åˆ**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ç›£è¦–

### ğŸ› ï¸ çµ±åˆæ©Ÿèƒ½
- **çµ±ä¸€ãƒ©ãƒ³ãƒãƒ£ãƒ¼**: å…¨æ©Ÿèƒ½ã‚’å˜ä¸€ã‚³ãƒãƒ³ãƒ‰ã§å®Ÿè¡Œ
- **YAMLè¨­å®š**: æŸ”è»Ÿãªè¨­å®šç®¡ç†
- **ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²**: TensorBoard/W&Bå¯¾å¿œ
- **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç†**: è‡ªå‹•ä¿å­˜ãƒ»å¾©å…ƒ

## ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
llm-complete-toolkit/
â”œâ”€â”€ main.py                      # çµ±åˆãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒãƒ£ãƒ¼
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml             # çµ±åˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ document_processing/         # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â”œâ”€â”€ pdf_parser.py       # PDFè§£æ
â”‚   â”‚   â””â”€â”€ markdown_parser.py  # Markdownè§£æ
â”‚   â””â”€â”€ converters/
â”‚       â””â”€â”€ lm_studio_converter.py  # LM Studioå¤‰æ›
â”œâ”€â”€ training_methods/            # å­¦ç¿’æ‰‹æ³•ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
â”‚   â”œâ”€â”€ reinforcement_learning/
â”‚   â”‚   â””â”€â”€ agents/
â”‚   â”‚       â”œâ”€â”€ ppo_agent.py    # PPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
â”‚   â”‚       â””â”€â”€ dqn_agent.py    # DQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
â”‚   â””â”€â”€ transfer_learning/
â”‚       â””â”€â”€ models/
â”‚           â”œâ”€â”€ lora_model.py   # LoRAãƒ¢ãƒ‡ãƒ«
â”‚           â””â”€â”€ qlora_model.py  # QLoRAãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ shared_utils/               # å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â”œâ”€â”€ data_loader.py         # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
â”‚   â”œâ”€â”€ training_utils.py      # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â””â”€â”€ file_utils.py          # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”œâ”€â”€ scripts/                   # å€‹åˆ¥ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   â”œâ”€â”€ train_lora.py
â”‚   â”œâ”€â”€ train_qlora.py
â”‚   â”œâ”€â”€ train_rl.py
â”‚   â””â”€â”€ download_models.py      # Hugging Faceãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
â”œâ”€â”€ models/                    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â”‚   â”œâ”€â”€ base_models/           # æœªå­¦ç¿’ï¼ˆäº‹å‰å­¦ç¿’æ¸ˆã¿ï¼‰ãƒ¢ãƒ‡ãƒ«
â”‚   â”œâ”€â”€ fine_tuned_models/     # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
â”‚   â””â”€â”€ trained_models/        # å¼·åŒ–å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ data/                      # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â”œâ”€â”€ outputs/                   # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â””â”€â”€ requirements.txt          # ä¾å­˜é–¢ä¿‚
```

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

#### ğŸ“¦ è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆæ¨å¥¨ï¼‰

**Linux/macOS:**
```bash
# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/lutelute/llm-complete-toolkit.git
cd llm-complete-toolkit

# è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Ÿè¡Œ
chmod +x install.sh
./install.sh
```

**Windows:**
```cmd
# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/lutelute/llm-complete-toolkit.git
cd llm-complete-toolkit

# è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Ÿè¡Œ
install.bat
```

#### ğŸ”§ æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/lutelute/llm-complete-toolkit.git
cd llm-complete-toolkit

# ä»®æƒ³ç’°å¢ƒã®ä½œæˆï¼ˆæ¨å¥¨ï¼‰
python -m venv venv
source venv/bin/activate  # Linux/macOS
# ã¾ãŸã¯
venv\Scripts\activate     # Windows

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt

# ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
python setup.py

# GPUæœ€é©åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
pip install flash-attn --no-build-isolation
```

#### ğŸ³ Dockeråˆ©ç”¨

```bash
# Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã®ãƒ“ãƒ«ãƒ‰
docker build -t llm-complete-toolkit .

# ã‚³ãƒ³ãƒ†ãƒŠã®å®Ÿè¡Œ
docker run -it --gpus all -v $(pwd):/workspace llm-complete-toolkit
```

### 2. ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

```bash
# äººæ°—ãƒ¢ãƒ‡ãƒ«ã®ä¸€è¦§è¡¨ç¤º
python scripts/download_models.py --list-popular

# å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
python scripts/download_models.py --model microsoft/DialoGPT-medium

# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
python scripts/download_models.py --model gpt2 --model microsoft/DialoGPT-small
```

### 3. ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ

```bash
python main.py create-samples
```

### 4. å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™

```bash
# å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®ï¼ˆå…¨å½¢å¼æ··åœ¨OKï¼‰
cp /path/to/your/mixed/files/* training_data/raw/

# å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºãƒ»å¤‰æ›
python main.py extract-training-data --split-data --instruction-format

# é…ç½®ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
ls -la training_data/raw/
```

### 5. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ï¼ˆå¾“æ¥ã®æ–¹æ³•ï¼‰

```bash
# PDFã‚„Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
python main.py extract data/ outputs/ --format jsonl --instruction-format
```

### 6. ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

```bash
# å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´
python main.py train-lora --train-data training_data/datasets/training/train.jsonl --eval-data training_data/datasets/validation/eval.jsonl

# QLoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ï¼‰
python main.py train-qlora --train-data training_data/datasets/training/train.jsonl

# PPOå¼·åŒ–å­¦ç¿’
python main.py train-rl --algorithm ppo

# DQNå¼·åŒ–å­¦ç¿’
python main.py train-rl --algorithm dqn
```

## ğŸ“š è©³ç´°ãªä½¿ç”¨æ–¹æ³•

### å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ç®¡ç†

#### ã‚µãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼
- **PDF**: .pdf
- **Markdown**: .md, .markdown
- **JSON**: .json, .jsonl
- **ãƒ†ã‚­ã‚¹ãƒˆ**: .txt

#### å‡¦ç†ä¾‹
```bash
# å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºï¼ˆå…¨å½¢å¼è‡ªå‹•æ¤œå‡ºï¼‰
python main.py extract-training-data

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ä»˜ãå‡¦ç†ï¼ˆæ¨å¥¨ï¼‰
python main.py extract-training-data --split-data --instruction-format

# ç‰¹å®šå½¢å¼ã®ã¿å‡¦ç†
python main.py extract-training-data --format pdf
python main.py extract-training-data --format json

# ã‚«ã‚¹ã‚¿ãƒ å…¥åŠ›ãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®š
python main.py extract-training-data --input-dir /path/to/your/documents

# è©³ç´°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
python main.py extract-training-data \
  --input-dir /path/to/your/documents \
  --output-dir training_data/processed \
  --output-format jsonl \
  --chunk-size 1024 \
  --instruction-format \
  --split-data \
  --train-ratio 0.8 \
  --val-ratio 0.1
```

### è»¢ç§»å­¦ç¿’

#### LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
```bash
python main.py train-lora \
  --train-data data/train.jsonl \
  --eval-data data/eval.jsonl \
  --model-name microsoft/DialoGPT-medium \
  --epochs 3 \
  --batch-size 4 \
  --learning-rate 2e-4
```

#### QLoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
```bash
python main.py train-qlora \
  --train-data data/train.jsonl \
  --model-name microsoft/DialoGPT-medium \
  --batch-size 1 \
  --epochs 3
```

### å¼·åŒ–å­¦ç¿’

#### PPOå­¦ç¿’
```bash
python main.py train-rl \
  --algorithm ppo \
  --episodes 1000 \
  --config configs/config.yaml
```

#### DQNå­¦ç¿’
```bash
python main.py train-rl \
  --algorithm dqn \
  --episodes 1000
```

## âš™ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

`configs/config.yaml`ã§å…¨ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã¾ã™ï¼š

```yaml
# å…±é€šè¨­å®š
common:
  device: "cuda"
  seed: 42
  log_level: "INFO"

# è»¢ç§»å­¦ç¿’è¨­å®š
transfer_learning:
  lora:
    model_name: "microsoft/DialoGPT-medium"
    lora_r: 16
    lora_alpha: 32
    training:
      num_train_epochs: 3
      learning_rate: 2e-4

# å¼·åŒ–å­¦ç¿’è¨­å®š
reinforcement_learning:
  ppo:
    learning_rate: 3e-4
    gamma: 0.99
    training:
      max_episodes: 1000
```

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢å¼

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆJSONLï¼‰
```json
{
  "instruction": "Pythonã§ãƒªã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
  "input": "",
  "output": "Pythonã§ãƒªã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹ã«ã¯ã€è§’æ‹¬å¼§[]ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ä¾‹: my_list = [1, 2, 3]"
}
```

### LM Studioå¤‰æ›ãƒ‡ãƒ¼ã‚¿
```json
{
  "text": "æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå†…å®¹",
  "source": "ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹",
  "title": "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒˆãƒ«",
  "type": "pdf",
  "chunk_id": 1,
  "total_chunks": 5,
  "metadata": {...}
}
```

## ğŸ¯ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†
- `training_data.jsonl`: LM Studioç”¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
- `instruction_data.jsonl`: ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³å½¢å¼ãƒ‡ãƒ¼ã‚¿

### LoRA/QLoRA
- `pytorch_model.bin`: LoRAé‡ã¿
- `adapter_config.json`: ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼è¨­å®š
- `training_config.yaml`: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š

### å¼·åŒ–å­¦ç¿’
- `{algorithm}_final.pt`: æœ€çµ‚ãƒ¢ãƒ‡ãƒ«
- `{algorithm}_episode_{n}.pt`: å®šæœŸä¿å­˜ãƒ¢ãƒ‡ãƒ«
- `training_curves.png`: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ›²ç·š

## ğŸ“ˆ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

### TensorBoard
```bash
tensorboard --logdir=outputs/logs/tensorboard
```

### Weights & Biases
è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§W&Bã‚’æœ‰åŠ¹åŒ–ï¼š
```yaml
logging:
  use_wandb: true
  wandb_project: "my-llm-project"
```

## ğŸ”§ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### æ–°ã—ã„ãƒ‘ãƒ¼ã‚µãƒ¼ã®è¿½åŠ 
```python
# document_processing/parsers/my_parser.py
class MyParser:
    def parse(self, file_path):
        # ã‚«ã‚¹ã‚¿ãƒ è§£æãƒ­ã‚¸ãƒƒã‚¯
        return documents
```

### ã‚«ã‚¹ã‚¿ãƒ å¼·åŒ–å­¦ç¿’ç’°å¢ƒ
```python
# training_methods/reinforcement_learning/environments/
class MyEnvironment:
    def reset(self):
        return initial_state
    
    def step(self, action):
        return next_state, reward, done, info
```

## ğŸ’¡ ä½¿ç”¨ä¾‹ã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

### 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰LLMãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¾ã§
```bash
# ã‚¹ãƒ†ãƒƒãƒ—1: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ
python main.py create-samples

# ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†
python main.py extract data/ outputs/ --instruction-format

# ã‚¹ãƒ†ãƒƒãƒ—3: LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
python main.py train-lora --train-data outputs/instruction_data.jsonl
```

### 2. å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡çš„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
```bash
# QLoRAã§å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
python main.py train-qlora \
  --train-data data/train.jsonl \
  --model-name microsoft/DialoGPT-large \
  --batch-size 1 \
  --epochs 2
```

### 3. å¼·åŒ–å­¦ç¿’ã§ã®ãƒãƒªã‚·ãƒ¼æœ€é©åŒ–
```bash
# PPOã§å®‰å®šã—ãŸãƒãƒªã‚·ãƒ¼å­¦ç¿’
python main.py train-rl --algorithm ppo --episodes 2000
```

## â— ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ğŸ“‹ ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ãƒã‚§ãƒƒã‚¯

#### Pythonç’°å¢ƒã®ç¢ºèª
```bash
# Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
python --version  # 3.8ä»¥ä¸Šå¿…è¦

# ä»®æƒ³ç’°å¢ƒã®ç¢ºèª
which python  # venvå†…ã®pythonã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã‹ç¢ºèª
```

#### GPUç’°å¢ƒã®ç¢ºèª
```bash
# CUDAç’°å¢ƒã®ç¢ºèª
nvidia-smi
python -c "import torch; print('CUDA:', torch.cuda.is_available())"

# Apple Silicon (MPS) ã®ç¢ºèª
python -c "import torch; print('MPS:', torch.backends.mps.is_available())"
```

### ğŸ”§ ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

#### 1. CUDA out of memory
```bash
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹
python main.py train-lora --train-data data/train.jsonl --batch-size 1

# QLoRAã‚’ä½¿ç”¨ã™ã‚‹ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ï¼‰
python main.py train-qlora --train-data data/train.jsonl --batch-size 1

# å‹¾é…ç´¯ç©ã‚’ä½¿ç”¨ã™ã‚‹
python main.py train-lora --train-data data/train.jsonl --batch-size 1 --gradient-accumulation-steps 8
```

#### 2. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼
```bash
# HuggingFaceã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
rm -rf ~/.cache/huggingface/

# ä¾å­˜é–¢ä¿‚ã‚’æ›´æ–°
pip install --upgrade transformers torch accelerate

# ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã‚’å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
python scripts/download_models.py --model microsoft/DialoGPT-medium
```

#### 3. ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼
```bash
# pip ã‚’æœ€æ–°ç‰ˆã«ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ
pip install --upgrade pip setuptools wheel

# ä¾å­˜é–¢ä¿‚ã‚’å€‹åˆ¥ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install torch transformers datasets

# requirements.txtã‚’æ®µéšçš„ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt --no-deps
pip install -r requirements.txt
```

#### 4. Flash Attention ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—
```bash
# CUDAç’°å¢ƒã§ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install flash-attn --no-build-isolation

# ç’°å¢ƒãŒåˆã‚ãªã„å ´åˆã¯ç„¡åŠ¹åŒ–
# requirements.txtã§ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
```

#### 5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å•é¡Œ
```bash
# CPUä½¿ç”¨æ•°ã‚’åˆ¶é™
export OMP_NUM_THREADS=4

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åˆ¶é™
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# æ··åˆç²¾åº¦ã‚’æœ‰åŠ¹åŒ–
python main.py train-lora --train-data data/train.jsonl --fp16
```

### ğŸ¥ è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
# ç’°å¢ƒè¨ºæ–­ã®å®Ÿè¡Œ
python -c "
import torch
import transformers
import sys

print('Python:', sys.version)
print('PyTorch:', torch.__version__)
print('Transformers:', transformers.__version__)
print('CUDA Available:', torch.cuda.is_available())
if torch.cuda.is_available():
    print('CUDA Version:', torch.version.cuda)
    print('GPU Count:', torch.cuda.device_count())
    print('GPU Name:', torch.cuda.get_device_name(0))
"
```

### ğŸ“ ã‚µãƒãƒ¼ãƒˆ

å•é¡ŒãŒè§£æ±ºã—ãªã„å ´åˆã¯ã€ä»¥ä¸‹ã®æƒ…å ±ã¨å…±ã«Issueã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

1. **ç’°å¢ƒæƒ…å ±**: OSã€Pythonç‰ˆã€GPUæƒ…å ±
2. **ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**: å®Œå…¨ãªã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
3. **å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰**: å®Ÿéš›ã«å®Ÿè¡Œã—ãŸã‚³ãƒãƒ³ãƒ‰
4. **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«**: ä½¿ç”¨ã—ãŸconfig.yamlã®å†…å®¹

```bash
# ç’°å¢ƒæƒ…å ±ã‚’å–å¾—
python setup.py --debug > debug_info.txt
```

### ğŸ”— å‚è€ƒãƒªãƒ³ã‚¯

- [PyTorch GPU ã‚µãƒãƒ¼ãƒˆ](https://pytorch.org/get-started/locally/)
- [HuggingFace Transformers ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://huggingface.co/docs/transformers)
- [PEFT (Parameter Efficient Fine-Tuning)](https://huggingface.co/docs/peft)
- [Stable Baselines3 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://stable-baselines3.readthedocs.io/)

## ğŸ¤ è²¢çŒ®

1. ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ•ã‚©ãƒ¼ã‚¯
2. æ©Ÿèƒ½ãƒ–ãƒ©ãƒ³ãƒã‚’ä½œæˆ (`git checkout -b feature/amazing-feature`)
3. å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆ (`git commit -m 'Add amazing feature'`)
4. ãƒ–ãƒ©ãƒ³ãƒã«ãƒ—ãƒƒã‚·ãƒ¥ (`git push origin feature/amazing-feature`)
5. ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

## ğŸ†˜ ã‚µãƒãƒ¼ãƒˆ

å•é¡Œã‚„è³ªå•ãŒã‚ã‚‹å ´åˆã¯ã€GitHubã®Issuesãƒšãƒ¼ã‚¸ã§å ±å‘Šã—ã¦ãã ã•ã„ã€‚

---

**LLM Complete Toolkit** - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‹ã‚‰æ©Ÿæ¢°å­¦ç¿’ã¾ã§ã€ä¸€è²«ã—ãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æä¾›ã™ã‚‹çµ±åˆãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆ ğŸš€