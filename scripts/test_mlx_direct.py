#!/usr/bin/env python3
"""
MLXã§ç›´æ¥ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import sys
from pathlib import Path

try:
    import mlx.core as mx
    import mlx.nn as nn
    from transformers import AutoTokenizer, AutoModelForCausalLM
    print("âœ… MLXã¨TransformersãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError as e:
    print(f"âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")
    sys.exit(1)

def test_model_with_mlx(model_path: str):
    """MLXæœ€é©åŒ–ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ"""
    print(f"\nğŸš€ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_path}")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")
    
    # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="mps",  # Apple Siliconæœ€é©åŒ–
        torch_dtype=mx.float32 if hasattr(mx, 'float32') else None
    )
    print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
    
    # GFMãƒ†ã‚¹ãƒˆè³ªå•
    test_prompts = [
        "GFMã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "ã‚°ãƒªãƒƒãƒ‰ãƒ•ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¤ãƒ³ãƒãƒ¼ã‚¿ãƒ¼ã®ç‰¹å¾´ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
        "ã“ã‚“ã«ã¡ã¯"
    ]
    
    print("\nğŸ§ª GFMãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™...")
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nğŸ“ è³ªå•{i}: {prompt}")
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        inputs = tokenizer.encode(prompt, return_tensors="pt")
        
        # MLXæœ€é©åŒ–ã§ç”Ÿæˆ
        with mx.no_grad() if hasattr(mx, 'no_grad') else nullcontext():
            outputs = model.generate(
                inputs,
                max_new_tokens=100,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # çµæœã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = response[len(prompt):].strip()
        
        print(f"ğŸ’¬ å›ç­”: {answer}")
        print("-" * 50)

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    model_path = "models/safetensors_models/GFM-DialoGPT-small-safetensors"
    
    if not Path(model_path).exists():
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
        print("ã¾ãšä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„:")
        print("python scripts/merge_lora_model.py --base-model models/base_models/microsoft_DialoGPT-small --lora-adapter models/fine_tuned_models/DialoGPT-small-GFM --output models/safetensors_models/GFM-DialoGPT-small-safetensors --device auto")
        sys.exit(1)
    
    print("ğŸ Apple Silicon (MLX) æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    try:
        test_model_with_mlx(model_path)
        print("\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº†!")
        print("\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±:")
        print("- Apple Silicon (M4) æœ€é©åŒ–æ¸ˆã¿")
        print("- çµ±åˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ´»ç”¨")
        print("- CPU+GPUå”èª¿å‡¦ç†")
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("MLXç›´æ¥å¤‰æ›ã¯ç¾åœ¨DialoGPTã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“")
        print("ä»£æ›¿æ¡ˆ: PyTorchã®MPSæœ€é©åŒ–ã‚’ä½¿ç”¨ä¸­")

if __name__ == "__main__":
    main()