#!/usr/bin/env python3
"""
å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import sys
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import time

def test_model(model_path: str, model_name: str):
    """ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""
    try:
        print(f"=== {model_name}ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
        print(f"ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {model_path}")
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        device = "mps" if torch.backends.mps.is_available() else "cpu"
        print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
        
        # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
        print("ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        start_time = time.time()
        
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if device == "mps" else torch.float32,
            device_map="auto" if device != "cpu" else None
        )
        
        if device == "mps":
            model = model.to(device)
        
        load_time = time.time() - start_time
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº† ({load_time:.2f}ç§’)")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¨ˆç®—
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
        print(f"è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}")
        
        # ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š
        if "GFM" in model_name:
            test_prompts = [
                "ã‚°ãƒªãƒƒãƒ‰ãƒ•ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¤ãƒ³ãƒãƒ¼ã‚¿ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
                "å†ç”Ÿå¯èƒ½ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®èª²é¡Œã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
                "ãƒã‚¤ã‚¯ãƒ­ã‚°ãƒªãƒƒãƒ‰ã®åˆ©ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ"
            ]
        else:
            test_prompts = [
                "Pythonã§printæ–‡ã‚’ä½¿ã£ã¦æŒ¨æ‹¶ã‚’å‡ºåŠ›ã™ã‚‹æ–¹æ³•ã‚’æ•™ãˆã¦",
                "æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
                "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®åŸºæœ¬ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„"
            ]
        
        # å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ãƒ†ã‚¹ãƒˆ
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\n--- ãƒ†ã‚¹ãƒˆ {i}: {prompt[:30]}... ---")
            
            try:
                # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
                inputs = tokenizer.encode(prompt, return_tensors="pt")
                if device == "mps":
                    inputs = inputs.to(device)
                
                # ç”Ÿæˆé–‹å§‹
                start_time = time.time()
                
                with torch.no_grad():
                    outputs = model.generate(
                        inputs,
                        max_new_tokens=50,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        pad_token_id=tokenizer.eos_token_id
                    )
                
                generation_time = time.time() - start_time
                
                # ãƒ‡ã‚³ãƒ¼ãƒ‰
                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                response = generated_text[len(prompt):].strip()
                
                print(f"å…¥åŠ›: {prompt}")
                print(f"å‡ºåŠ›: {response}")
                print(f"ç”Ÿæˆæ™‚é–“: {generation_time:.2f}ç§’")
                
                # ãƒˆãƒ¼ã‚¯ãƒ³/ç§’ã®è¨ˆç®—
                new_tokens = len(outputs[0]) - len(inputs[0])
                tokens_per_sec = new_tokens / generation_time if generation_time > 0 else 0
                print(f"ç”Ÿæˆé€Ÿåº¦: {tokens_per_sec:.1f} tokens/sec")
                
            except Exception as e:
                print(f"âŒ ãƒ†ã‚¹ãƒˆ{i}ã§ã‚¨ãƒ©ãƒ¼: {e}")
        
        print(f"âœ… {model_name}ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆå®Œäº†")
        return True
        
    except Exception as e:
        print(f"âŒ {model_name}ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def benchmark_models():
    """å…¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    print("=== å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ===")
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèª
    model_paths = {
        "minimal_test": "./lm_studio_models/minimal_test_merged",
        "GFM": "./lm_studio_models/DialoGPT-small-GFM_merged"
    }
    
    available_models = []
    for name, path in model_paths.items():
        if os.path.exists(path):
            available_models.append((name, path))
        else:
            print(f"ã‚¹ã‚­ãƒƒãƒ—: {name} (ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {path})")
    
    if not available_models:
        print("âŒ ãƒ†ã‚¹ãƒˆå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    print(f"\nåˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«:")
    for i, (name, path) in enumerate(available_models):
        print(f"  {i+1}. {name} ({path})")
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    print(f"\n=== ã‚·ã‚¹ãƒ†ãƒ æƒ…å ± ===")
    print(f"Python: {sys.version}")
    print(f"PyTorch: {torch.__version__}")
    print(f"MPSåˆ©ç”¨å¯èƒ½: {torch.backends.mps.is_available()}")
    if torch.backends.mps.is_available():
        print(f"MPSãƒ“ãƒ«ãƒˆ: {torch.backends.mps.is_built()}")
    
    # å…¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ
    successful_tests = []
    
    for name, model_path in available_models:
        print(f"\n{'='*60}")
        success = test_model(model_path, name)
        if success:
            successful_tests.append(name)
        print(f"{'='*60}")
    
    # çµæœã‚µãƒãƒªãƒ¼
    print(f"\n=== ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚µãƒãƒªãƒ¼ ===")
    
    if successful_tests:
        print(f"âœ… æˆåŠŸã—ãŸãƒ†ã‚¹ãƒˆ: {len(successful_tests)}å€‹")
        for name in successful_tests:
            print(f"  - {name}")
        
        print(f"\nğŸš€ æ¨å¥¨ä½¿ç”¨æ–¹æ³•:")
        print(f"1. ç›´æ¥Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰åˆ©ç”¨")
        print(f"2. ä¸Šè¨˜ã®ãƒ†ã‚¹ãƒˆçµæœã‚’å‚è€ƒã«é©åˆ‡ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨")
        print(f"3. Apple Siliconã®å ´åˆã¯MPSãƒ‡ãƒã‚¤ã‚¹ã§é«˜é€Ÿå‹•ä½œ")
        
        print(f"\nğŸ’¡ æœ€é©åŒ–ã®ãƒã‚¤ãƒ³ãƒˆ:")
        print(f"- ğŸ MPSãƒ‡ãƒã‚¤ã‚¹åˆ©ç”¨ã§Apple Siliconæœ€é©åŒ–")
        print(f"- ğŸ§  float16ç²¾åº¦ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–")
        print(f"- âš¡ ãƒãƒƒãƒå‡¦ç†ã§é«˜é€ŸåŒ–å¯èƒ½")
        print(f"- ğŸ¯ ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã§é«˜å“è³ªå›ç­”")
        
    else:
        print(f"âŒ æˆåŠŸã—ãŸãƒ†ã‚¹ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“")
    
    print(f"\nğŸ‰ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†ï¼")

def main():
    benchmark_models()

if __name__ == "__main__":
    main()