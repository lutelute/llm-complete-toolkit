# ğŸš€ LLM Complete Toolkit

PDFã‚„Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã¨ã€å¼·åŒ–å­¦ç¿’ãƒ»è»¢ç§»å­¦ç¿’ã‚’ç”¨ã„ãŸLLMãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®çµ±åˆãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆ

## âœ¨ ä¸»ãªç‰¹å¾´

### ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†
- **PDFè§£æ**: PDFplumber + PyPDF2ã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
- **Markdownè§£æ**: ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼å¯¾å¿œã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰²
- **LM Studioå¤‰æ›**: JSONLå½¢å¼ã§ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
- **è‡ªå‹•ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²**: æœ€é©ãªã‚µã‚¤ã‚ºã§ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²

### ğŸ§  è»¢ç§»å­¦ç¿’
- **LoRA**: åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- **QLoRA**: é‡å­åŒ–ã«ã‚ˆã‚‹è¶…ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
- **è‡ªå‹•ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™**: ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³å½¢å¼å¯¾å¿œ
- **HuggingFaceçµ±åˆ**: æœ€æ–°ã®Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªå¯¾å¿œ

### ğŸ¯ å¼·åŒ–å­¦ç¿’
- **PPO**: å®‰å®šã—ãŸãƒãƒªã‚·ãƒ¼å‹¾é…æ‰‹æ³•
- **DQN**: ä¾¡å€¤ãƒ™ãƒ¼ã‚¹å­¦ç¿’ï¼ˆDueling DQNå¯¾å¿œï¼‰
- **ã‚«ã‚¹ã‚¿ãƒ ç’°å¢ƒ**: ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ç’°å¢ƒã®ã‚µãƒ³ãƒ—ãƒ«å®Ÿè£…
- **TensorBoardçµ±åˆ**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ç›£è¦–

### ğŸ› ï¸ çµ±åˆæ©Ÿèƒ½
- **çµ±ä¸€ãƒ©ãƒ³ãƒãƒ£ãƒ¼**: å…¨æ©Ÿèƒ½ã‚’å˜ä¸€ã‚³ãƒãƒ³ãƒ‰ã§å®Ÿè¡Œ
- **YAMLè¨­å®š**: æŸ”è»Ÿãªè¨­å®šç®¡ç†
- **ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²**: TensorBoard/W&Bå¯¾å¿œ
- **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç†**: è‡ªå‹•ä¿å­˜ãƒ»å¾©å…ƒ

## ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
llm-complete-toolkit/
â”œâ”€â”€ main.py                      # çµ±åˆãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒãƒ£ãƒ¼
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml             # çµ±åˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ document_processing/         # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â”œâ”€â”€ pdf_parser.py       # PDFè§£æ
â”‚   â”‚   â””â”€â”€ markdown_parser.py  # Markdownè§£æ
â”‚   â””â”€â”€ converters/
â”‚       â””â”€â”€ lm_studio_converter.py  # LM Studioå¤‰æ›
â”œâ”€â”€ training_methods/            # å­¦ç¿’æ‰‹æ³•ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
â”‚   â”œâ”€â”€ reinforcement_learning/
â”‚   â”‚   â””â”€â”€ agents/
â”‚   â”‚       â”œâ”€â”€ ppo_agent.py    # PPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
â”‚   â”‚       â””â”€â”€ dqn_agent.py    # DQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
â”‚   â””â”€â”€ transfer_learning/
â”‚       â””â”€â”€ models/
â”‚           â”œâ”€â”€ lora_model.py   # LoRAãƒ¢ãƒ‡ãƒ«
â”‚           â””â”€â”€ qlora_model.py  # QLoRAãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ shared_utils/               # å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â”œâ”€â”€ data_loader.py         # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
â”‚   â”œâ”€â”€ training_utils.py      # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â””â”€â”€ file_utils.py          # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”œâ”€â”€ scripts/                   # å€‹åˆ¥ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   â”œâ”€â”€ train_lora.py
â”‚   â”œâ”€â”€ train_qlora.py
â”‚   â”œâ”€â”€ train_rl.py
â”‚   â””â”€â”€ download_models.py      # Hugging Faceãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
â”œâ”€â”€ models/                    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â”‚   â”œâ”€â”€ base_models/           # æœªå­¦ç¿’ï¼ˆäº‹å‰å­¦ç¿’æ¸ˆã¿ï¼‰ãƒ¢ãƒ‡ãƒ«
â”‚   â”œâ”€â”€ fine_tuned_models/     # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
â”‚   â””â”€â”€ trained_models/        # å¼·åŒ–å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ data/                      # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â”œâ”€â”€ outputs/                   # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â””â”€â”€ requirements.txt          # ä¾å­˜é–¢ä¿‚
```

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
git clone <repository-url>
cd llm-complete-toolkit

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# Flash Attention (ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€GPUã§ã®é«˜é€ŸåŒ–)
pip install flash-attn --no-build-isolation
```

### 2. ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

```bash
# äººæ°—ãƒ¢ãƒ‡ãƒ«ã®ä¸€è¦§è¡¨ç¤º
python scripts/download_models.py --list-popular

# å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
python scripts/download_models.py --model microsoft/DialoGPT-medium

# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
python scripts/download_models.py --model gpt2 --model microsoft/DialoGPT-small
```

### 3. ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ

```bash
python main.py create-samples
```

### 4. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†

```bash
# PDFã‚„Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
python main.py extract data/ outputs/ --format jsonl --instruction-format
```

### 5. ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

```bash
# LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
python main.py train-lora --train-data data/train.jsonl

# QLoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ï¼‰
python main.py train-qlora --train-data data/train.jsonl

# PPOå¼·åŒ–å­¦ç¿’
python main.py train-rl --algorithm ppo

# DQNå¼·åŒ–å­¦ç¿’
python main.py train-rl --algorithm dqn
```

## ğŸ“š è©³ç´°ãªä½¿ç”¨æ–¹æ³•

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†

#### ã‚µãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼
- **PDF**: .pdf
- **Markdown**: .md, .markdown
- **ãƒ†ã‚­ã‚¹ãƒˆ**: .txt

#### å‡¦ç†ä¾‹
```bash
# åŸºæœ¬çš„ãªæŠ½å‡º
python main.py extract documents/ outputs/

# è©³ç´°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
python main.py extract documents/ outputs/ \
  --format jsonl \
  --chunk-size 1024 \
  --instruction-format
```

### è»¢ç§»å­¦ç¿’

#### LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
```bash
python main.py train-lora \
  --train-data data/train.jsonl \
  --eval-data data/eval.jsonl \
  --model-name microsoft/DialoGPT-medium \
  --epochs 3 \
  --batch-size 4 \
  --learning-rate 2e-4
```

#### QLoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
```bash
python main.py train-qlora \
  --train-data data/train.jsonl \
  --model-name microsoft/DialoGPT-medium \
  --batch-size 1 \
  --epochs 3
```

### å¼·åŒ–å­¦ç¿’

#### PPOå­¦ç¿’
```bash
python main.py train-rl \
  --algorithm ppo \
  --episodes 1000 \
  --config configs/config.yaml
```

#### DQNå­¦ç¿’
```bash
python main.py train-rl \
  --algorithm dqn \
  --episodes 1000
```

## âš™ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

`configs/config.yaml`ã§å…¨ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã¾ã™ï¼š

```yaml
# å…±é€šè¨­å®š
common:
  device: "cuda"
  seed: 42
  log_level: "INFO"

# è»¢ç§»å­¦ç¿’è¨­å®š
transfer_learning:
  lora:
    model_name: "microsoft/DialoGPT-medium"
    lora_r: 16
    lora_alpha: 32
    training:
      num_train_epochs: 3
      learning_rate: 2e-4

# å¼·åŒ–å­¦ç¿’è¨­å®š
reinforcement_learning:
  ppo:
    learning_rate: 3e-4
    gamma: 0.99
    training:
      max_episodes: 1000
```

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢å¼

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆJSONLï¼‰
```json
{
  "instruction": "Pythonã§ãƒªã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
  "input": "",
  "output": "Pythonã§ãƒªã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹ã«ã¯ã€è§’æ‹¬å¼§[]ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ä¾‹: my_list = [1, 2, 3]"
}
```

### LM Studioå¤‰æ›ãƒ‡ãƒ¼ã‚¿
```json
{
  "text": "æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå†…å®¹",
  "source": "ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹",
  "title": "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒˆãƒ«",
  "type": "pdf",
  "chunk_id": 1,
  "total_chunks": 5,
  "metadata": {...}
}
```

## ğŸ¯ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†
- `training_data.jsonl`: LM Studioç”¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
- `instruction_data.jsonl`: ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³å½¢å¼ãƒ‡ãƒ¼ã‚¿

### LoRA/QLoRA
- `pytorch_model.bin`: LoRAé‡ã¿
- `adapter_config.json`: ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼è¨­å®š
- `training_config.yaml`: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š

### å¼·åŒ–å­¦ç¿’
- `{algorithm}_final.pt`: æœ€çµ‚ãƒ¢ãƒ‡ãƒ«
- `{algorithm}_episode_{n}.pt`: å®šæœŸä¿å­˜ãƒ¢ãƒ‡ãƒ«
- `training_curves.png`: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ›²ç·š

## ğŸ“ˆ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

### TensorBoard
```bash
tensorboard --logdir=outputs/logs/tensorboard
```

### Weights & Biases
è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§W&Bã‚’æœ‰åŠ¹åŒ–ï¼š
```yaml
logging:
  use_wandb: true
  wandb_project: "my-llm-project"
```

## ğŸ”§ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### æ–°ã—ã„ãƒ‘ãƒ¼ã‚µãƒ¼ã®è¿½åŠ 
```python
# document_processing/parsers/my_parser.py
class MyParser:
    def parse(self, file_path):
        # ã‚«ã‚¹ã‚¿ãƒ è§£æãƒ­ã‚¸ãƒƒã‚¯
        return documents
```

### ã‚«ã‚¹ã‚¿ãƒ å¼·åŒ–å­¦ç¿’ç’°å¢ƒ
```python
# training_methods/reinforcement_learning/environments/
class MyEnvironment:
    def reset(self):
        return initial_state
    
    def step(self, action):
        return next_state, reward, done, info
```

## ğŸ’¡ ä½¿ç”¨ä¾‹ã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

### 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰LLMãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¾ã§
```bash
# ã‚¹ãƒ†ãƒƒãƒ—1: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ
python main.py create-samples

# ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†
python main.py extract data/ outputs/ --instruction-format

# ã‚¹ãƒ†ãƒƒãƒ—3: LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
python main.py train-lora --train-data outputs/instruction_data.jsonl
```

### 2. å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡çš„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
```bash
# QLoRAã§å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
python main.py train-qlora \
  --train-data data/train.jsonl \
  --model-name microsoft/DialoGPT-large \
  --batch-size 1 \
  --epochs 2
```

### 3. å¼·åŒ–å­¦ç¿’ã§ã®ãƒãƒªã‚·ãƒ¼æœ€é©åŒ–
```bash
# PPOã§å®‰å®šã—ãŸãƒãƒªã‚·ãƒ¼å­¦ç¿’
python main.py train-rl --algorithm ppo --episodes 2000
```

## â— ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

#### CUDA out of memory
```bash
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹
python main.py train-qlora --train-data data/train.jsonl --batch-size 1

# QLoRAã‚’ä½¿ç”¨ã™ã‚‹
python main.py train-qlora --train-data data/train.jsonl
```

#### ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼
```bash
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
rm -rf ~/.cache/huggingface/

# ä¾å­˜é–¢ä¿‚ã‚’æ›´æ–°
pip install --upgrade transformers torch
```

#### ãƒ¡ãƒ¢ãƒªä¸è¶³
- QLoRAã‚’ä½¿ç”¨
- gradient_accumulation_stepsã‚’å¢—ã‚„ã™
- per_device_train_batch_sizeã‚’æ¸›ã‚‰ã™

## ğŸ¤ è²¢çŒ®

1. ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ•ã‚©ãƒ¼ã‚¯
2. æ©Ÿèƒ½ãƒ–ãƒ©ãƒ³ãƒã‚’ä½œæˆ (`git checkout -b feature/amazing-feature`)
3. å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆ (`git commit -m 'Add amazing feature'`)
4. ãƒ–ãƒ©ãƒ³ãƒã«ãƒ—ãƒƒã‚·ãƒ¥ (`git push origin feature/amazing-feature`)
5. ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

## ğŸ†˜ ã‚µãƒãƒ¼ãƒˆ

å•é¡Œã‚„è³ªå•ãŒã‚ã‚‹å ´åˆã¯ã€GitHubã®Issuesãƒšãƒ¼ã‚¸ã§å ±å‘Šã—ã¦ãã ã•ã„ã€‚

---

**LLM Complete Toolkit** - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‹ã‚‰æ©Ÿæ¢°å­¦ç¿’ã¾ã§ã€ä¸€è²«ã—ãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æä¾›ã™ã‚‹çµ±åˆãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆ ğŸš€