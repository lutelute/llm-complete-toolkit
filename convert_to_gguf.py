#!/usr/bin/env python3
"""
å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import sys
import os
import subprocess
from pathlib import Path
import json

def check_llama_cpp():
    """llama.cppã®å­˜åœ¨ç¢ºèªã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    try:
        # llama.cppã®convert.pyã‚’æ¢ã™
        llama_cpp_paths = [
            "/usr/local/bin/llama.cpp",
            "/opt/homebrew/bin/llama.cpp", 
            "~/llama.cpp",
            "./llama.cpp"
        ]
        
        for path in llama_cpp_paths:
            expanded_path = os.path.expanduser(path)
            if os.path.exists(expanded_path):
                convert_script = os.path.join(expanded_path, "convert_hf_to_gguf.py")
                if os.path.exists(convert_script):
                    print(f"âœ… llama.cpp found: {expanded_path}")
                    return convert_script
        
        print("âŒ llama.cppãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("ä»¥ä¸‹ã®æ‰‹é †ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ï¼š")
        print("1. git clone https://github.com/ggerganov/llama.cpp")
        print("2. cd llama.cpp")
        print("3. make")
        return None
        
    except Exception as e:
        print(f"llama.cppç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        return None

def install_llama_cpp():
    """llama.cppã‚’è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    try:
        print("=== llama.cppè‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«é–‹å§‹ ===")
        
        # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚¯ãƒ­ãƒ¼ãƒ³
        if not os.path.exists("./llama.cpp"):
            print("llama.cppã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ä¸­...")
            subprocess.run([
                "git", "clone", 
                "https://github.com/ggerganov/llama.cpp.git"
            ], check=True)
        
        # ãƒ“ãƒ«ãƒ‰
        print("llama.cppã‚’ãƒ“ãƒ«ãƒ‰ä¸­...")
        subprocess.run([
            "make", "-C", "./llama.cpp"
        ], check=True)
        
        convert_script = "./llama.cpp/convert_hf_to_gguf.py"
        if os.path.exists(convert_script):
            print("âœ… llama.cppã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†")
            return convert_script
        else:
            print("âŒ convert_hf_to_gguf.pyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
    except subprocess.CalledProcessError as e:
        print(f"âŒ llama.cppã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        return None
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def convert_to_gguf(model_path: str, output_path: str, convert_script: str):
    """ãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›"""
    try:
        print(f"=== GGUFå¤‰æ›é–‹å§‹ ===")
        print(f"å…¥åŠ›: {model_path}")
        print(f"å‡ºåŠ›: {output_path}")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # å¤‰æ›å®Ÿè¡Œ
        cmd = [
            "python3", convert_script,
            model_path,
            "--outdir", os.path.dirname(output_path),
            "--outfile", os.path.basename(output_path)
        ]
        
        print(f"å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            print("âœ… GGUFå¤‰æ›å®Œäº†")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ç¢ºèª
            if os.path.exists(output_path):
                size_mb = os.path.getsize(output_path) / (1024 * 1024)
                print(f"å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path} ({size_mb:.1f} MB)")
                return True
            else:
                print("âŒ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
        else:
            print(f"âŒ å¤‰æ›ã‚¨ãƒ©ãƒ¼:")
            print(f"stdout: {result.stdout}")
            print(f"stderr: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        return False

def quantize_gguf(gguf_path: str, quantization_type: str = "Q4_K_M"):
    """GGUFãƒ¢ãƒ‡ãƒ«ã‚’é‡å­åŒ–"""
    try:
        print(f"=== GGUFé‡å­åŒ–é–‹å§‹ ({quantization_type}) ===")
        
        # é‡å­åŒ–å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«å
        base_name = os.path.splitext(gguf_path)[0]
        quantized_path = f"{base_name}-{quantization_type}.gguf"
        
        # llama.cppã®é‡å­åŒ–ãƒ„ãƒ¼ãƒ«
        quantize_tool = "./llama.cpp/llama-quantize"
        
        if not os.path.exists(quantize_tool):
            print("âŒ llama-quantizeãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        
        cmd = [
            quantize_tool,
            gguf_path,
            quantized_path,
            quantization_type
        ]
        
        print(f"å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            print("âœ… é‡å­åŒ–å®Œäº†")
            
            if os.path.exists(quantized_path):
                size_mb = os.path.getsize(quantized_path) / (1024 * 1024)
                print(f"é‡å­åŒ–ãƒ•ã‚¡ã‚¤ãƒ«: {quantized_path} ({size_mb:.1f} MB)")
                return quantized_path
            else:
                print("âŒ é‡å­åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
        else:
            print(f"âŒ é‡å­åŒ–ã‚¨ãƒ©ãƒ¼:")
            print(f"stdout: {result.stdout}")
            print(f"stderr: {result.stderr}")
            return None
            
    except Exception as e:
        print(f"âŒ é‡å­åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        return None

def copy_to_lm_studio(gguf_path: str, model_name: str):
    """GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’LM Studioãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼"""
    try:
        import shutil
        
        lm_studio_dir = os.path.expanduser("~/Library/Application Support/lm-studio/models")
        target_dir = os.path.join(lm_studio_dir, model_name)
        
        print(f"=== LM Studioã«ã‚³ãƒ”ãƒ¼ ===")
        print(f"ã‚³ãƒ”ãƒ¼å…ˆ: {target_dir}")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs(target_dir, exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
        target_path = os.path.join(target_dir, os.path.basename(gguf_path))
        shutil.copy2(gguf_path, target_path)
        
        print(f"âœ… ã‚³ãƒ”ãƒ¼å®Œäº†: {target_path}")
        return target_path
        
    except Exception as e:
        print(f"âŒ ã‚³ãƒ”ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def main():
    print("=== ãƒ¢ãƒ‡ãƒ«GGUFå¤‰æ›ãƒ„ãƒ¼ãƒ« ===")
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèª
    model_paths = {
        "minimal_test": "./lm_studio_models/minimal_test_merged",
        "GFM": "./lm_studio_models/DialoGPT-small-GFM_merged"
    }
    
    available_models = []
    for name, path in model_paths.items():
        if os.path.exists(path):
            available_models.append((name, path))
    
    if not available_models:
        print("âŒ å¤‰æ›å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("å…ˆã«merge_for_lmstudio.pyã¾ãŸã¯merge_gfm_model.pyã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return
    
    print(f"\nåˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«:")
    for i, (name, path) in enumerate(available_models):
        print(f"  {i+1}. {name} ({path})")
    
    # llama.cppã®ç¢ºèª
    convert_script = check_llama_cpp()
    if not convert_script:
        print("\nllama.cppã‚’è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã‹ï¼Ÿ (y/n)")
        if input().lower() == 'y':
            convert_script = install_llama_cpp()
            if not convert_script:
                print("âŒ llama.cppã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
        else:
            print("âŒ llama.cppãŒå¿…è¦ã§ã™")
            return
    
    # å…¨ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›
    for name, model_path in available_models:
        print(f"\n=== {name}ãƒ¢ãƒ‡ãƒ«ã®å¤‰æ›é–‹å§‹ ===")
        
        # GGUFå¤‰æ›
        gguf_dir = "./gguf_models"
        gguf_path = os.path.join(gguf_dir, f"{name}.gguf")
        
        success = convert_to_gguf(model_path, gguf_path, convert_script)
        
        if success:
            # é‡å­åŒ–
            quantized_path = quantize_gguf(gguf_path, "Q4_K_M")
            
            if quantized_path:
                # LM Studioã«ã‚³ãƒ”ãƒ¼
                lm_studio_path = copy_to_lm_studio(quantized_path, f"{name}-gguf")
                
                if lm_studio_path:
                    print(f"âœ… {name}ãƒ¢ãƒ‡ãƒ«ã®GGUFå¤‰æ›å®Œäº†")
                    print(f"LM Studioãƒ‘ã‚¹: {lm_studio_path}")
        
        print("-" * 50)
    
    print(f"\nğŸ‰ GGUFå¤‰æ›å®Œäº†ï¼")
    print(f"\nğŸš€ LM Studioä½¿ç”¨æ–¹æ³•:")
    print(f"1. LM Studioã‚’é–‹ã")
    print(f"2. ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½:")
    for name, _ in available_models:
        print(f"   - {name}-gguf")
    print(f"\nğŸ’¡ GGUFã®åˆ©ç‚¹:")
    print(f"- ã‚ˆã‚Šé«˜é€Ÿãªæ¨è«–")
    print(f"- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›")
    print(f"- CPUæ¨è«–ã®æœ€é©åŒ–")

if __name__ == "__main__":
    main()