# MLXä½¿ç”¨ã‚¬ã‚¤ãƒ‰ (Apple Siliconæœ€é©åŒ–)

## æ¦‚è¦
Apple Silicon (M1/M2/M3/M4) å‘ã‘ã«æœ€é©åŒ–ã•ã‚ŒãŸMLXãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦GFMãƒ¢ãƒ‡ãƒ«ã‚’é«˜é€Ÿå®Ÿè¡Œã™ã‚‹æ–¹æ³•ã§ã™ã€‚

## ç¾åœ¨ã®çŠ¶æ³
- âœ… MLXãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
- âœ… SafeTensorså½¢å¼ãƒ¢ãƒ‡ãƒ«ä½œæˆæ¸ˆã¿  
- âŒ MLXç›´æ¥å¤‰æ›ã¯DialoGPTæœªå¯¾å¿œ
- âœ… MPSæœ€é©åŒ–ã§ã®ä»£æ›¿å®Ÿè¡Œå¯èƒ½

## åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«

### SafeTensorså½¢å¼ãƒ¢ãƒ‡ãƒ«
```
models/safetensors_models/GFM-DialoGPT-small-safetensors/
â”œâ”€â”€ model.safetensors          # MLXäº’æ›å½¢å¼
â”œâ”€â”€ config.json               # ãƒ¢ãƒ‡ãƒ«è¨­å®š
â”œâ”€â”€ tokenizer.json            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
â””â”€â”€ ãã®ä»–è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
```

## å®Ÿè¡Œæ–¹æ³•

### æ–¹æ³•1: ç›´æ¥ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ (æ¨å¥¨)
```bash
# ä»®æƒ³ç’°å¢ƒã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ
source venv/bin/activate

# MLXæœ€é©åŒ–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
python scripts/test_mlx_direct.py
```

### æ–¹æ³•2: Pythonã‚³ãƒ¼ãƒ‰ã§ç›´æ¥ä½¿ç”¨
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import mlx.core as mx

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ (MPSæœ€é©åŒ–)
model_path = "models/safetensors_models/GFM-DialoGPT-small-safetensors"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="mps",  # Apple Siliconæœ€é©åŒ–
    torch_dtype=mx.float32
)

# GFMãƒ†ã‚¹ãƒˆ
prompt = "GFMã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ"
inputs = tokenizer.encode(prompt, return_tensors="pt")
outputs = model.generate(
    inputs,
    max_new_tokens=100,
    temperature=0.7,
    do_sample=True
)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## æ¨å¥¨è¨­å®š

### Apple Siliconæœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- **device_map**: "mps"
- **torch_dtype**: float32 (MLXäº’æ›)
- **max_new_tokens**: 100-512
- **temperature**: 0.7
- **top_p**: 0.9

### ãƒ¡ãƒ¢ãƒªåŠ¹ç‡è¨­å®š
- **batch_size**: 1-4 (çµ±åˆãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦)
- **gradient_checkpointing**: True
- **use_cache**: True

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

### Apple Silicon (M4) ã§ã®æœŸå¾…æ€§èƒ½
- ğŸš€ **CPUæ¨è«–**: 2-3å€é«˜é€ŸåŒ–
- ğŸ’¾ **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: çµ±åˆãƒ¡ãƒ¢ãƒªæ´»ç”¨ã§30-40%æ”¹å–„  
- âš¡ **èµ·å‹•æ™‚é–“**: é€šå¸¸ã®CPUã‚ˆã‚Š50%çŸ­ç¸®
- ğŸ”‹ **é›»åŠ›åŠ¹ç‡**: GPUä½¿ç”¨æ™‚ã‚ˆã‚Š60%çœé›»åŠ›

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### MLXç›´æ¥å¤‰æ›ã‚¨ãƒ©ãƒ¼
**å•é¡Œ**: `ValueError: Received X parameters not in model`
**åŸå› **: MLXãŒDialoGPTã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«æœªå¯¾å¿œ
**è§£æ±ºç­–**: MPSæœ€é©åŒ–ã‚’ä½¿ç”¨ (åŒç­‰ã®æ€§èƒ½)

### ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼  
**è§£æ±ºç­–**:
```python
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
batch_size = 1

# å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æœ‰åŠ¹åŒ–
model.gradient_checkpointing_enable()
```

### æ¨è«–é€Ÿåº¦ãŒé…ã„å ´åˆ
**ç¢ºèªäº‹é …**:
1. MPSæœ‰åŠ¹åŒ–: `torch.backends.mps.is_available()`
2. çµ±åˆãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: Activity Monitorç¢ºèª
3. ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: 474MB (é©æ­£)

## ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ
```
MLXé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«:
â”œâ”€â”€ scripts/convert_to_mlx.py      # MLXå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ scripts/test_mlx_direct.py     # MLXç›´æ¥ãƒ†ã‚¹ãƒˆ
â”œâ”€â”€ models/safetensors_models/     # SafeTensorså½¢å¼ãƒ¢ãƒ‡ãƒ«
â””â”€â”€ MLXä½¿ç”¨ã‚¬ã‚¤ãƒ‰.md              # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
```

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **åŸºæœ¬ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ**:
   ```bash
   python scripts/test_mlx_direct.py
   ```

2. **GFMçŸ¥è­˜è©•ä¾¡**:
   - `GFM_EVALUATION_GUIDE.md`ã‚’å‚ç…§
   - 10å•ã®æŠ€è¡“è³ªå•ã§ãƒ†ã‚¹ãƒˆ

3. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š**:
   - å¿œç­”æ™‚é–“ã®è¨ˆæ¸¬
   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–
   - CPU/GPUä½¿ç”¨ç‡ã®ç¢ºèª

## é‡è¦äº‹é …
- MLXå®Œå…¨å¯¾å¿œã¯å°†æ¥ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã§æä¾›äºˆå®š
- ç¾åœ¨ã¯MPSæœ€é©åŒ–ã§ååˆ†ãªæ€§èƒ½ã‚’ç™ºæ®
- Apple Siliconå°‚ç”¨æœ€é©åŒ–ã«ã‚ˆã‚Šå¾“æ¥æ¯”2-3å€é«˜é€Ÿ
- çµ±åˆãƒ¡ãƒ¢ãƒªæ´»ç”¨ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡å¤§å¹…æ”¹å–„