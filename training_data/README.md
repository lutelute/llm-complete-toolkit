# å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿æ ¼ç´ãƒ•ã‚©ãƒ«ãƒ€

ã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ã€LLMãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹ãŸã‚ã®ãƒ•ã‚©ãƒ«ãƒ€ã§ã™ã€‚

## ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ 

```
training_data/
â”œâ”€â”€ raw/                    # ç”Ÿãƒ‡ãƒ¼ã‚¿æ ¼ç´ï¼ˆå…¨å½¢å¼æ··åœ¨OKï¼‰
â”‚   â”œâ”€â”€ document1.pdf      # PDFãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â”œâ”€â”€ notes.md           # Markdownãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â”œâ”€â”€ data.json          # JSONãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â”œâ”€â”€ text_file.txt      # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â””â”€â”€ mixed_files...     # è¤‡æ•°å½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ··åœ¨
â”œâ”€â”€ processed/             # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
â”‚   â”œâ”€â”€ training_data.jsonl        # å¤‰æ›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
â”‚   â””â”€â”€ instruction_data.jsonl     # ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³å½¢å¼
â””â”€â”€ datasets/              # å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆåˆ†å‰²æ¸ˆã¿ï¼‰
    â”œâ”€â”€ training/          # è¨“ç·´ç”¨ãƒ‡ãƒ¼ã‚¿
    â”‚   â””â”€â”€ train.jsonl
    â”œâ”€â”€ validation/        # æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿
    â”‚   â””â”€â”€ eval.jsonl
    â””â”€â”€ test/              # ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿
        â””â”€â”€ test.jsonl
```

## ğŸ“„ å¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼

### å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`raw/`ãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ï¼‰
- **PDF**: `.pdf`
- **Markdown**: `.md`, `.markdown`
- **ãƒ†ã‚­ã‚¹ãƒˆ**: `.txt`
- **JSON**: `.json`, `.jsonl`

### å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰
- **JSONL**: å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿å½¢å¼
- **CSV**: åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿å½¢å¼
- **TXT**: ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. ãƒ‡ãƒ¼ã‚¿ã®é…ç½®
```bash
# å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€ã¤ã®ãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ï¼ˆå½¢å¼æ··åœ¨OKï¼‰
cp /path/to/your/mixed/files/* training_data/raw/

# ã¾ãŸã¯å€‹åˆ¥ã«ã‚³ãƒ”ãƒ¼
cp /path/to/your/documents/*.pdf training_data/raw/
cp /path/to/your/notes/*.md training_data/raw/
cp /path/to/your/data/*.json training_data/raw/
cp /path/to/your/texts/*.txt training_data/raw/

# ãƒ•ã‚©ãƒ«ãƒ€ã”ã¨ã‚³ãƒ”ãƒ¼ã‚‚å¯èƒ½
cp -r /path/to/your/document_folder/* training_data/raw/
```

### 2. ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
```bash
# å…¨å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•å‡¦ç†ï¼ˆæ¨å¥¨ï¼‰
python main.py extract-training-data

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ä»˜ãå‡¦ç†
python main.py extract-training-data --split-data --instruction-format

# ç‰¹å®šã®å½¢å¼ã®ã¿å‡¦ç†
python main.py extract-training-data --format pdf
python main.py extract-training-data --format markdown
python main.py extract-training-data --format text
python main.py extract-training-data --format json

# å‡ºåŠ›å½¢å¼ã‚’æŒ‡å®š
python main.py extract-training-data --output-format jsonl
python main.py extract-training-data --output-format csv
```

### 3. å­¦ç¿’ã®å®Ÿè¡Œ
```bash
# å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
python main.py train-lora --train-data training_data/datasets/training/train.jsonl

# æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ä»˜ãã§å­¦ç¿’
python main.py train-lora \
  --train-data training_data/datasets/training/train.jsonl \
  --eval-data training_data/datasets/validation/eval.jsonl
```

## ğŸ“‹ ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®ä¾‹

### PDFãƒ•ã‚¡ã‚¤ãƒ«
```
training_data/raw/pdf/
â”œâ”€â”€ document1.pdf
â”œâ”€â”€ document2.pdf
â””â”€â”€ research_paper.pdf
```

### Markdownãƒ•ã‚¡ã‚¤ãƒ«
```
training_data/raw/markdown/
â”œâ”€â”€ tutorial.md
â”œâ”€â”€ documentation.md
â””â”€â”€ notes.markdown
```

### JSONãƒ•ã‚¡ã‚¤ãƒ«
```json
{
  "instruction": "è³ªå•ã®å†…å®¹",
  "input": "è¿½åŠ ã®å…¥åŠ›ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
  "output": "æœŸå¾…ã•ã‚Œã‚‹å›ç­”"
}
```

### JSONLãƒ•ã‚¡ã‚¤ãƒ«
```jsonl
{"instruction": "è³ªå•1", "input": "", "output": "å›ç­”1"}
{"instruction": "è³ªå•2", "input": "", "output": "å›ç­”2"}
```

## ğŸ”„ å‡¦ç†ãƒ•ãƒ­ãƒ¼

1. **ãƒ‡ãƒ¼ã‚¿é…ç½®**: `raw/`ãƒ•ã‚©ãƒ«ãƒ€ã«å…ƒãƒ‡ãƒ¼ã‚¿ã‚’é…ç½®
2. **æŠ½å‡ºå‡¦ç†**: å„å½¢å¼ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º â†’ `processed/extracted/`
3. **ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²**: é©åˆ‡ãªã‚µã‚¤ã‚ºã«åˆ†å‰² â†’ `processed/chunks/`
4. **ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›**: å­¦ç¿’ç”¨å½¢å¼ã«å¤‰æ› â†’ `processed/formatted/`
5. **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ**: è¨“ç·´/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰² â†’ `datasets/`

## âš™ï¸ è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³

### `configs/config.yaml`ã§ã®è¨­å®š
```yaml
training_data:
  raw_data_dir: "./training_data/raw"
  processed_data_dir: "./training_data/processed"
  datasets_dir: "./training_data/datasets"
  
  # ãƒãƒ£ãƒ³ã‚¯è¨­å®š
  chunk_size: 512
  chunk_overlap: 50
  
  # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²æ¯”ç‡
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  
  # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè¨­å®š
  output_format: "jsonl"
  instruction_format: true
```

## ğŸ” ãƒ‡ãƒ¼ã‚¿ç¢ºèª

### å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
```bash
# å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§
ls -la training_data/processed/

# ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç¢ºèª
head -5 training_data/datasets/training/train.jsonl

# ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã®ç¢ºèª
python -c "
import json
with open('training_data/datasets/training/train.jsonl', 'r') as f:
    data = [json.loads(line) for line in f]
    print(f'Total samples: {len(data)}')
    print(f'Average text length: {sum(len(d[\"output\"]) for d in data) / len(data):.1f}')
"
```

## ğŸš¨ æ³¨æ„äº‹é …

1. **å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«**: å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
2. **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†æ™‚ã¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«æ³¨æ„ã—ã¦ãã ã•ã„
3. **ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**: æ—¥æœ¬èªãƒ•ã‚¡ã‚¤ãƒ«ã¯UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¨å¥¨ã—ã¾ã™
4. **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**: é‡è¦ãªãƒ‡ãƒ¼ã‚¿ã¯äº‹å‰ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å–ã£ã¦ãã ã•ã„

## ğŸ’¡ ãƒ’ãƒ³ãƒˆ

- å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¾ãšãƒ†ã‚¹ãƒˆã—ã¦ã‹ã‚‰å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹
- å‡¦ç†æ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹ãŸã‚ã€ä¸è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã¯äº‹å‰ã«é™¤å¤–ã™ã‚‹
- å®šæœŸçš„ã«ä¸­é–“çµæœã‚’ç¢ºèªã—ã¦ã€æœŸå¾…é€šã‚Šã®å‡¦ç†ãŒè¡Œã‚ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã™ã‚‹